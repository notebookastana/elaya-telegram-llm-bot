import os
import logging
from typing import List, Dict
from pathlib import Path

try:
    from langchain_community.document_loaders import (
        PyPDFLoader,
        Docx2txtLoader,
        TextLoader
    )
    # –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–º–ø–æ—Ä—Ç–æ–≤ LangChain
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    from langchain_community.embeddings import HuggingFaceEmbeddings
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø–∞–∫–µ—Ç langchain-chroma
    try:
        from langchain_chroma import Chroma
    except ImportError:
        from langchain_community.vectorstores import Chroma
        
    RAG_AVAILABLE = True
except ImportError as e:
    RAG_AVAILABLE = False
    RAG_IMPORT_ERROR = str(e)
    logging.warning(f"‚ö†Ô∏è RAG –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {e}")

logger = logging.getLogger("rag_manager")

class RAGManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –ø–æ–∏—Å–∫–æ–º —á–µ—Ä–µ–∑ RAG"""
    
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            persist_directory: –ü–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        """
        self.persist_directory = persist_directory
        self.embeddings = None
        self.vectorstore = None
        self.is_initialized = False
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(persist_directory, exist_ok=True)
        
        logger.info(f"üìÅ RAG directory: {persist_directory}")
    
    def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç embedding –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        if not RAG_AVAILABLE:
            logger.error(f"‚ùå RAG –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {RAG_IMPORT_ERROR}")
            logger.error("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install langchain langchain-community chromadb sentence-transformers pypdf docx2txt")
            return False
        
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏...")
            logger.info("‚è≥ –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–π–º—ë—Ç ~1-2 –º–∏–Ω—É—Ç—ã (—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ~500MB)...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            logger.info("‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            if os.path.exists(os.path.join(self.persist_directory, "chroma.sqlite3")):
                logger.info("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                doc_count = len(self.vectorstore.get()['ids'])
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {doc_count}")
            else:
                logger.info("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                logger.info("‚úÖ –ë–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ (–ø–æ–∫–∞ –ø—É—Å—Ç–∞—è)")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            return False
    
    def load_document(self, file_path: str) -> List:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        if not RAG_AVAILABLE:
            logger.error("‚ùå RAG –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!")
            return []
        
        file_path = Path(file_path)
        
        if not file_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return []
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            ext = file_path.suffix.lower()
            
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞: {file_path.name} ({ext})")
            
            if ext == '.pdf':
                loader = PyPDFLoader(str(file_path))
            elif ext == '.docx':
                loader = Docx2txtLoader(str(file_path))
            elif ext == '.txt':
                loader = TextLoader(str(file_path), encoding='utf-8')
            else:
                logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}")
                logger.info("üí° –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: .pdf, .docx, .txt")
                return []
            
            documents = loader.load()
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü/—Ä–∞–∑–¥–µ–ª–æ–≤: {len(documents)}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            
            chunks = text_splitter.split_documents(documents)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            for chunk in chunks:
                chunk.metadata['source_file'] = file_path.name
                chunk.metadata['file_type'] = ext
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            return chunks
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
            return []
    
    def add_documents(self, file_paths: List[str]) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        if not self.is_initialized or not self.vectorstore:
            logger.error("‚ùå RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ initialize() —Å–Ω–∞—á–∞–ª–∞")
            logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏...")
            if not self.initialize():
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG!")
                return 0
        
        total_chunks = 0
        successful_files = 0
        failed_files = 0
        
        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"üìÅ [{i}/{len(file_paths)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞...")
            chunks = self.load_document(file_path)
            
            if chunks:
                try:
                    logger.info(f"üíæ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
                    self.vectorstore.add_documents(chunks)
                    total_chunks += len(chunks)
                    successful_files += 1
                    logger.info(f"‚úÖ [{i}/{len(file_paths)}] –£—Å–ø–µ—à–Ω–æ: {Path(file_path).name} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
                except Exception as e:
                    logger.exception(f"‚ùå [{i}/{len(file_paths)}] –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É: {e}")
                    failed_files += 1
            else:
                logger.warning(f"‚ö†Ô∏è [{i}/{len(file_paths)}] –ü—Ä–æ–ø—É—â–µ–Ω: {Path(file_path).name}")
                failed_files += 1
        
        if total_chunks > 0:
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
            try:
                # –í –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ langchain-chroma persist –Ω–µ –Ω—É–∂–µ–Ω (–∞–≤—Ç–æ-—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)
                if hasattr(self.vectorstore, 'persist'):
                    self.vectorstore.persist()
                logger.info(f"‚úÖ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–∞–≤—Ç–æ-—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ)")
            
            logger.info(f"üìä –ò—Ç–æ–≥–æ: —É—Å–ø–µ—à–Ω–æ={successful_files}, –æ—à–∏–±–æ–∫={failed_files}, —á–∞–Ω–∫–æ–≤={total_chunks}")
        
        return total_chunks
    
    def search(self, query: str, k: int = 5) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.is_initialized:
            logger.error("‚ùå RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
            return []
        
        if not self.vectorstore:
            logger.error("‚ùå –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
            return []
        
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query[:50]}...'")
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for doc, score in results:
                formatted_results.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source_file', 'Unknown'),
                    'score': float(score),
                    'metadata': doc.metadata
                })
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(formatted_results)}")
            return formatted_results
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.is_initialized:
            return {
                'total_chunks': 0,
                'total_sources': 0,
                'sources': {},
                'status': 'not_initialized'
            }
        
        if not self.vectorstore:
            return {
                'total_chunks': 0,
                'total_sources': 0,
                'sources': {},
                'status': 'vectorstore_error'
            }
        
        try:
            data = self.vectorstore.get()
            doc_count = len(data['ids'])
            
            # –ü–æ–¥—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            sources = {}
            for metadata in data['metadatas']:
                source = metadata.get('source_file', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
            
            return {
                'total_chunks': doc_count,
                'total_sources': len(sources),
                'sources': sources,
                'status': 'ready'
            }
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {
                'total_chunks': 0,
                'total_sources': 0,
                'sources': {},
                'status': 'error',
                'error': str(e)
            }
    
    def clear_database(self):
        """–û—á–∏—â–∞–µ—Ç –≤—Å—é –±–∞–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.is_initialized:
            logger.error("‚ùå RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
            return False
        
        try:
            if self.vectorstore:
                # –£–¥–∞–ª—è–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                data = self.vectorstore.get()
                if data['ids']:
                    self.vectorstore.delete(data['ids'])
                    if hasattr(self.vectorstore, 'persist'):
                        self.vectorstore.persist()
                    logger.info("üóëÔ∏è –ë–∞–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—á–∏—â–µ–Ω–∞")
                    return True
                else:
                    logger.info("‚ÑπÔ∏è –ë–∞–∑–∞ —É–∂–µ –ø—É—Å—Ç–∞")
                    return True
            return False
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –±–∞–∑—ã: {e}")
            return False


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä (—Å–æ–∑–¥–∞—ë—Ç—Å—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ)
rag_manager = RAGManager()